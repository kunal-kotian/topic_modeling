{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T02:06:14.259690Z",
     "start_time": "2018-08-06T02:06:14.252366Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "from collections import Counter\n",
    "from itertools import groupby\n",
    "from itertools import chain\n",
    "\n",
    "import spacy\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A: Loading the JSON Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T08:35:19.312982Z",
     "start_time": "2018-08-05T08:35:18.402538Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('../data/corpus.txt') as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T08:35:25.789748Z",
     "start_time": "2018-08-05T08:35:25.784195Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents in the JSON file corpus.txt:  10000\n"
     ]
    }
   ],
   "source": [
    "print('number of documents in the JSON file corpus.txt: ', len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.1: Extracting Relevant Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a single document from the JSON file to understand its structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T08:35:33.182968Z",
     "start_time": "2018-08-05T08:35:33.165717Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': {'string': 'Chicago Tribune'},\n",
       " 'crawlName': {'string': 'chicago_tribue_business'},\n",
       " 'date': 1507161600000,\n",
       " 'html': '',\n",
       " 'humanLanguage': {'string': 'en'},\n",
       " 'pageUrl': 'http://www.chicagotribune.com/news/local/breaking/ct-gunman-reserved-two-rooms-at-blackstone-photos-20171005-photogallery.html',\n",
       " 'siteName': {'string': 'chicagotribune.com'},\n",
       " 'tags': [{'count': 1,\n",
       "   'label': 'The Blackstone Hotel',\n",
       "   'rdfTypes': [],\n",
       "   'score': 0.38,\n",
       "   'uri': 'http://dbpedia.org/page/The_Blackstone_Hotel'},\n",
       "  {'count': 1,\n",
       "   'label': 'Chicago Police Department',\n",
       "   'rdfTypes': [],\n",
       "   'score': 0.37,\n",
       "   'uri': 'http://dbpedia.org/page/Chicago_Police_Department'},\n",
       "  {'count': 1,\n",
       "   'label': 'music festival',\n",
       "   'rdfTypes': [],\n",
       "   'score': 0.24,\n",
       "   'uri': 'http://dbpedia.org/page/Music_festival'},\n",
       "  {'count': 1,\n",
       "   'label': 'mass shooting',\n",
       "   'rdfTypes': [],\n",
       "   'score': 0.19,\n",
       "   'uri': 'http://dbpedia.org/page/Mass_shooting'},\n",
       "  {'count': 1,\n",
       "   'label': 'Lollapalooza',\n",
       "   'rdfTypes': [],\n",
       "   'score': 0.18,\n",
       "   'uri': 'http://dbpedia.org/page/Lollapalooza'},\n",
       "  {'count': 1,\n",
       "   'label': 'Las Vegas',\n",
       "   'rdfTypes': ['http://dbpedia.org/ontology/Place'],\n",
       "   'score': 0.12,\n",
       "   'uri': 'http://dbpedia.org/page/Las_Vegas'}],\n",
       " 'text': \"Chicago police are investigating whether Stephen Paddock, the gunman in the Las Vegas mass shooting, booked rooms at the Blackstone Hotel during this summer's Lollapalooza music festival, held across the street in Grant Park.\",\n",
       " 'thumbnailUrl': {'string': 'http://www.trbimg.com/img-59d66185/turbine/chi-blackstone05-ct0060623053-20171005/750/750x422'},\n",
       " 'title': 'Gunman reserved two rooms at Blackstone'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears from the sample document above that each article's title and main body are stored in the document's `title` and `text` attributes as strings respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity checks to confirm whether the JSON object `data` maintains the structure seen above:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T08:35:40.910540Z",
     "start_time": "2018-08-05T08:35:40.892088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 most frequent counts of article lengths (no. of characters in the article):\n",
      "[(1, 7793), (0, 43), (1068, 4), (2714, 4), (482, 4)]\n"
     ]
    }
   ],
   "source": [
    "article_lengths = [len(doc['text']) for doc in data]   # list of the lengths of each article\n",
    "article_lengths_count = Counter(article_lengths)       # dict mapping article length to frequency of occurrence\n",
    "print('5 most frequent counts of article lengths (no. of characters in the article):')\n",
    "print(article_lengths_count.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are `7793` documents with article length `1` and `43` with length `0`.  This seems unusual and must be investigated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T08:35:47.816352Z",
     "start_time": "2018-08-05T08:35:47.806090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A document having length of text == 0: format -> (title, text, URL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"Craig Robinson and Adam Scott buddy up in Fox's supernatural comedy 'Ghosted'\",\n",
       " '',\n",
       " 'http://www.chicagotribune.com/entertainment/tv/la-et-st-ghosted-review-20170930-story.html')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('A document having length of text == 0: format -> (title, text, URL)')\n",
    "(next(iter(((doc['title'] , doc['text'], doc['pageUrl']) for doc in data if len(doc['text']) == 0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample document above has its article text missing.  The URL associated with the article shows that the full article text is available to read.  The same observation applies to a few other similar documents with zero article lengths that were explored.  It might be possible to acquire the full article text for such articles later.  Since article title constitutes a useful signal, these articles will be retained for topic modeling.  \n",
    "\n",
    "Let's check if any documents have missing article text as well as title; if any are found, they must be deleted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T08:35:54.630051Z",
     "start_time": "2018-08-05T08:35:54.619477Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of documents missing both title as well as text: 0\n"
     ]
    }
   ],
   "source": [
    "print('No. of documents missing both title as well as text: ', end='')\n",
    "print(len([doc for doc in data if len(doc['text']) == 0 and len(doc['title']) == 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's look at a sample document with article length `1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T08:36:00.679781Z",
     "start_time": "2018-08-05T08:36:00.668348Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A document having length of text == 1: format -> (title, text, URL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'string': 'Losses for banks and smaller companies take US stocks lower | The Sacramento Bee'},\n",
       " {'string': \"U.S. stock indexes are slipping back from record highs Tuesday as banks and small companies fall. Travel booking sites Priceline and TripAdvisor are taking steep losses following their third-quarter reports and retailers are falling too. Companies that pay big dividends, including utilities, are making gains. Oil prices are down slightly after they jumped to two-year highs a day ago.\\nKEEPING SCORE: The Standard & Poor's 500 index lost 4 points, or 0.2 percent, to 2,586 as of 3 p.m. Eastern time. The Dow Jones industrial average slipped 26 points, or 0.1 percent, to 23,521. The Nasdaq composite fell 26 points, or 0.4 percent, to 6,759. Smaller companies were on track for their worst loss since early August. The Russell 2000 index tumbled 18 points, or 1.2 percent, to 1,479 as Wall Street continued to watch for signs of progress by House Republicans on their proposed tax cuts. Investors feel smaller and more U.S.-focused companies could benefit the most from a corporate tax cut.\\nIN DECLINE: Banks fell along with bond yields and interest rates. Both have moved lower over the last few days, which reduces the profits banks make from lending. The yield on the 10-year Treasury note slipped to 2.31 percent from 2.32 percent.\\nJPMorgan Chase fell $2.15, or 2.1 percent, to $98.63 and U.S. Bancorp lost $1.31, or 2.4 percent, to $53.54. Bank of America gave up 70 cents, or 2.5 percent, to $27.05. First Financial Bancshares, a Texas-based bank, fell $1.10, or 2.4 percent, to $44.45.\\nAmong smaller companies, Red Robin Gourmet Burgers plunged after it slashed its profit forecast. It pointed to higher labor costs and said it will stop opening new locations at the end of its next fiscal year. The stock lost $18.75, or 28 percent, to $48.30. Consumer products distributor Core-Mark fell $3.5, or 10.4 percent, to $30.20 after it cut its outlook.\\nTRAVELING MOOD: Travel website TripAdvisor plunged after its third-quarter revenue fell short of analyst estimates. Booking service Priceline Group had a better-than-expected quarter, but its forecasts for the current quarter disappointed Wall Street. Analysts said the company is spending a lot of money on advertising, but that may pay off with increased market share. TripAdvisor sank $8.62, or 21.8 percent, to $30.91 and Priceline lost $254.20, or 13.4 percent, to $1,648.80.\\nTECH RALLY FADES: Chipmaker Skyworks Solutions fell $6.03, or 5.2 percent, to $110.30 and Microchip Technology gave up $4.08, or 4.3 percent, to $91.28 after their quarterly reports. Semiconductor makers jumped Monday on deal reports in the industry. The losses took technology companies slightly lower and threatened to end an eight-day winning streak. The stocks have been setting record highs throughout the run and technology companies have done far better than the rest of the market all year.\\nMOVING UP: Household goods makers, utilities, and other companies that pay big dividends did better than the rest of the market. Drugstore and pharmacy benefits company CVS Health jumped $2.10, or 3.1 percent, to $68.90 to recover some of its recent losses. Shopping mall operator GGP climbed $1.56, or 8.2 percent, to $20.57.\\nReal estate, household goods and phone companies have lagged far behind the S&P 500 this year. The stocks are generally seen as cautious investments, and investors have been betting on improved economic growth rather than looking for safety.\\nSLIMMED DOWN: Weight loss company Weight Watchers continued to surge after it raised its forecasts for the year. Its stock rose $7.66, or 17.1 percent, to $52.46. It started the year at $11.45.\\nMALLIN-CRUSHED: Drugmaker Mallinckrodt plunged after it said sales of its costly HP Acthar gel have been hurt because fewer prescriptions are being filled. It said revenue from the drug will decline in the fourth quarter. The company also reported weaker sales of generic drugs. Already trading at all-time lows, the stock dropped $11.14, or 35.7 percent, to $20.04.\\nOIL: Benchmark U.S. crude fell 15 cents to $57.20 a barrel in New York. Brent crude, used to price international oils, dipped 58 cents to $63.69 a barrel in London. Oil prices rose about 3 percent Monday and hit two-year highs after a wave of arrests of princes and other officials in Saudi Arabia. Investors wondered if the upheaval could affect oil supplies and prices.\\nWholesale gasoline lost 1 cent to $1.82 a gallon. Heating oil fell 2 cents to $1.92 a gallon. Natural gas rose 2 cents to $3.15 per 1,000 cubic feet.\\nMETALS: Gold lost $5.80 to $1,275.80 an ounce. Silver fell 30 cents to $16.94 an ounce. Copper declined 7 cents to $3.09 a pound.\\nCURRENCIES: The dollar rose to 113.86 yen from 113.77 yen. The euro fell to $1.1588 from $1.1606.\\nOVERSEAS: European stocks fell. The British FTSE 100 and the German DAX each shed 0.7 percent. The CAC 40 in France lost 0.5 percent. Tokyo's Nikkei 225 jumped 1.7 percent and Hong Kong's Hang Seng advanced 1.3 percent. In Seoul, the Kospi lost 0.2 percent.\\nNever miss a local story.\\nSign up today for a free 30 day free trial of unlimited digital access.\\nSUBSCRIBE NOW\"},\n",
       " {'string': 'http://www.sacbee.com/news/business/article183179826.html'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('A document having length of text == 1: format -> (title, text, URL)')\n",
    "next(iter(((doc['title'] , doc['text'], doc['pageUrl']) for doc in data if len(doc['text']) == 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of the JSON document above is different from the first sample document.  This must be taken into account while extracting article titles and text from the JSON object `data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The fields of interest in the JSON documents are: `text` and `title`.**  \n",
    "\n",
    "Note:  \n",
    "The `tags` field is an array of entities extracted from based on text analysis by Diffbot (reference: https://www.diffbot.com/dev/docs/article/).  Each entity has a label (its name) and a relevance score.  However, a cursory exploration of these tags for a few documents revealed that some of the entities identified by Diffbot are not relevant to the article.  Hence, the `tags` field was not used in this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T08:36:06.806485Z",
     "start_time": "2018-08-05T08:36:06.798600Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_title_and_text(doc):\n",
    "    \"\"\"Returns a tuple of strings representing the article's title and text from the JSON document doc.\"\"\"\n",
    "    title = doc['title']\n",
    "    text = doc['text']\n",
    "    if type(text) is dict:\n",
    "        title = title['string']\n",
    "        text = text['string']\n",
    "    return title, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T08:36:13.459267Z",
     "start_time": "2018-08-05T08:36:13.434500Z"
    }
   },
   "outputs": [],
   "source": [
    "articles = []\n",
    "\n",
    "# initialize tracking of document index in the JSON array.\n",
    "# document index can be used to map any article to all \n",
    "# attributes available in the original JSON object 'data'.\n",
    "ind = 0\n",
    "for doc in data:\n",
    "    title, text = get_title_and_text(doc)\n",
    "    articles.append([ind, title, text])\n",
    "    ind += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity check:  **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T08:36:22.490717Z",
     "start_time": "2018-08-05T08:36:22.481419Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of documents with article length == 0 after text extraction: 0\n"
     ]
    }
   ],
   "source": [
    "print('No. of documents with article length == 0 after text extraction: ', end='')\n",
    "print(len([art for art in articles if len(art[2]) == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T08:36:31.800854Z",
     "start_time": "2018-08-05T08:36:31.658585Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the list 'articles' containing relevant information in the format:\n",
    "# [[document index in the original JSON array, article title, article text]]\n",
    "with open('../data/articles.pkl', 'wb') as file:\n",
    "    pickle.dump(articles, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T23:33:43.989653Z",
     "start_time": "2018-08-05T23:33:43.834976Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../data/articles.pkl', 'rb') as file:\n",
    "    articles = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B: Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, phrase detection is performed using the `Gensim` library.  To work with `Gensim`, each article's text must be transformed into a list of sentences, with each sentence being a list of word tokens.  This process is performed using the `SpaCy` NLP library in this section.  \n",
    "\n",
    "**Storing parts-of-speech:**  \n",
    "The final part of the phrase detection process in the next section involves filtering the identified bigrams and trigrams based on whether or not they match specific part-of-speech templates/patterns.  For this purpose, along with storing word tokens, their correspoding parts-of-speech are also stored in this section.\n",
    "\n",
    "**Lemmatization:**  \n",
    "Lemmatization converts a word to its base form, using knowledge of the part of speech of that word.  This helps in normalizing the text, and hence the lemmatized forms of the word tokens have been stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T23:56:44.499040Z",
     "start_time": "2018-08-05T23:56:41.241274Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')   # load SpaCy's default NLP model for English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.1: Tokenization of the Articles' Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T09:00:04.805614Z",
     "start_time": "2018-08-05T08:36:59.389968Z"
    }
   },
   "outputs": [],
   "source": [
    "# text_index_all yields (text, index) tuples as required by .pipe() later:\n",
    "# the article text yielded has the title joined to it\n",
    "# index is the document index in the original JSON array\n",
    "text_index_all = (('. '.join([title, text]), index) for index, title, text in articles)\n",
    "\n",
    "# generator yielding parsed SpaCy Doc objects, which are sequences of tokens:\n",
    "docs = nlp.pipe(text_index_all, as_tuples=True, batch_size=1000)\n",
    "\n",
    "# unigram_sents_pos will store lists of lemmatized tokens and their parts-of-speech (pos) for each sentence\n",
    "# format: [[document index, lemmatized tokens list, tokens' pos tag list], ...]\n",
    "unigram_sents_pos = []\n",
    "\n",
    "for parsed_text, index in docs:\n",
    "    for sent in parsed_text.sents:\n",
    "        # lemmatize tokens & save corresponding pos tags after filtering whitespace and punctuations\n",
    "        tokenized_sent = [(token.lemma_, token.pos_) for token in sent if not (token.is_space or token.is_punct)]\n",
    "        if len(tokenized_sent) != 0:\n",
    "            # separate out lemmatized tokens and pos tags\n",
    "            tokens, pos = list(zip(*tokenized_sent))\n",
    "            unigram_sents_pos.append([index, tokens, pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T09:04:45.771707Z",
     "start_time": "2018-08-05T09:04:43.000616Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../data/unigram_sents_pos.pkl', 'wb') as file:\n",
    "    pickle.dump(unigram_sents_pos, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T23:48:51.015818Z",
     "start_time": "2018-08-05T23:48:49.105728Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../data/unigram_sents_pos.pkl', 'rb') as file:\n",
    "    unigram_sents_pos = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`unigram_sents_pos` is a list of lists, with each constituent list representing a **sentence** in an article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be helpful to see the structure of `unigram_sents_pos` to understand the rest of the code.  Let's look at the representation of the article with index # `2` after tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T18:46:34.457001Z",
     "start_time": "2018-08-05T18:46:34.440294Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2,\n",
       "  'Gunman reserved two rooms at Blackstone',\n",
       "  \"Chicago police are investigating whether Stephen Paddock, the gunman in the Las Vegas mass shooting, booked rooms at the Blackstone Hotel during this summer's Lollapalooza music festival, held across the street in Grant Park.\"]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[art for art in articles if art[0] == 2]   # 3rd article in the JSON array (at index 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T18:46:43.222790Z",
     "start_time": "2018-08-05T18:46:43.189923Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2,\n",
       "  ('gunman', 'reserve', 'two', 'room', 'at', 'blackstone'),\n",
       "  ('PROPN', 'VERB', 'NUM', 'NOUN', 'ADP', 'PROPN')],\n",
       " [2,\n",
       "  ('chicago',\n",
       "   'police',\n",
       "   'be',\n",
       "   'investigate',\n",
       "   'whether',\n",
       "   'stephen',\n",
       "   'paddock',\n",
       "   'the',\n",
       "   'gunman',\n",
       "   'in',\n",
       "   'the',\n",
       "   'las',\n",
       "   'vegas',\n",
       "   'mass',\n",
       "   'shooting',\n",
       "   'book',\n",
       "   'room',\n",
       "   'at',\n",
       "   'the',\n",
       "   'blackstone',\n",
       "   'hotel',\n",
       "   'during',\n",
       "   'this',\n",
       "   'summer',\n",
       "   \"'s\",\n",
       "   'lollapalooza',\n",
       "   'music',\n",
       "   'festival',\n",
       "   'hold',\n",
       "   'across',\n",
       "   'the',\n",
       "   'street',\n",
       "   'in',\n",
       "   'grant',\n",
       "   'park'),\n",
       "  ('PROPN',\n",
       "   'NOUN',\n",
       "   'VERB',\n",
       "   'VERB',\n",
       "   'ADP',\n",
       "   'PROPN',\n",
       "   'PROPN',\n",
       "   'DET',\n",
       "   'NOUN',\n",
       "   'ADP',\n",
       "   'DET',\n",
       "   'PROPN',\n",
       "   'PROPN',\n",
       "   'NOUN',\n",
       "   'NOUN',\n",
       "   'VERB',\n",
       "   'NOUN',\n",
       "   'ADP',\n",
       "   'DET',\n",
       "   'PROPN',\n",
       "   'PROPN',\n",
       "   'ADP',\n",
       "   'DET',\n",
       "   'NOUN',\n",
       "   'PART',\n",
       "   'PROPN',\n",
       "   'NOUN',\n",
       "   'NOUN',\n",
       "   'VERB',\n",
       "   'ADP',\n",
       "   'DET',\n",
       "   'NOUN',\n",
       "   'ADP',\n",
       "   'PROPN',\n",
       "   'PROPN')]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the 3rd article's representation after tokenization \n",
    "[sent for sent in unigram_sents_pos if sent[0] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.2: Phrase Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally occurring bigram and trigram phrases (e.g. 'Bank of America') are identified in this section using the `Gensim` module.  The method for phrase detection used in this section relies on the calculation of the Normalized Pointwise Mutual Information (NPMI) score [ref: G. Bouma, “Normalized (pointwise) mutual information in collocation extraction,” Proceedings of GSCL, pp. 31–40, 2009.].  The higher the NPMI score for a set of two word tokens, the greater the likelihood of those words being part of a phrase.  All pairs of words with NPMI greater than a specified threshold are treated as phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokens in phrases are joined using `__`**:  \n",
    "Double underscores are used in the code below (via the `delimiter` argument supplied to `Phrases`) to join tokens that are parts of phrases.  The more commonly seen glue character `_` (single underscore) is not used here because the text contains several instances of `_`, e.g. as a part of twitter handles, hashtags, etc.  Using a double underscore - which does not appear anywhere in the tokens from the original text - avoids a mixup between paired words and unpaired words that contain an underscore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T23:51:27.105132Z",
     "start_time": "2018-08-05T23:49:25.273302Z"
    }
   },
   "outputs": [],
   "source": [
    "# gensim's phrase detector expects an iterable of sentences, \n",
    "# with each sentence being a list of word tokens\n",
    "unigram_sentences = [tokens for index, tokens, pos in unigram_sents_pos]\n",
    "\n",
    "# common_terms (stop words) passed to gensim's phrase detector are \n",
    "# ignored when determining the frequency count based NPMI score of the \n",
    "# phrases that they are a part of. Thus, their presence between two \n",
    "# words won’t hinder detection of phrases like “Bank of America”.\n",
    "common_terms = ['the', 'and', 'or', 'of', 'in', 'at', 'on']\n",
    "\n",
    "# Train a first-order phrase detector\n",
    "bigram_model = Phrases(unigram_sentences, threshold=0.6, scoring='npmi', common_terms=common_terms, delimiter=b'__')\n",
    "bigram_phraser = Phraser(bigram_model)   # object to apply bigram model to tokens\n",
    "bigram_sentences = bigram_phraser[unigram_sentences]\n",
    "\n",
    "# Train a second-order phrase detector\n",
    "trigram_model = Phrases(bigram_sentences, threshold=0.5, scoring='npmi', common_terms=common_terms, delimiter=b'__')\n",
    "trigram_phraser = Phraser(trigram_model)  # object to apply trigram model to tokens\n",
    "trigram_sentences = trigram_phraser[bigram_sentences]\n",
    "# convert to list (from gensim's TransformedCorpus type) for processing downstream\n",
    "trigram_sentences = list(trigram_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T23:52:21.823728Z",
     "start_time": "2018-08-05T23:52:21.400495Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('do__not', 9379),\n",
       " ('more__than', 3601),\n",
       " ('such__as', 2070),\n",
       " ('as__well', 2020),\n",
       " ('last__year', 1830),\n",
       " ('per__cent', 1355),\n",
       " ('new__york', 1244),\n",
       " ('united__states', 1156),\n",
       " ('year__ago', 888),\n",
       " ('last__week', 857),\n",
       " ('sign__up', 776),\n",
       " ('talk__about', 764),\n",
       " ('white__house', 717),\n",
       " ('more__information', 683),\n",
       " ('high__school', 655),\n",
       " ('social__medium', 603),\n",
       " ('long__term', 590),\n",
       " ('earlier__this', 561),\n",
       " ('north__korea', 554),\n",
       " ('less__than', 550)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paired_words = (word for sent in trigram_sentences for word in sent if '__' in word)\n",
    "paired_words_frq = Counter(paired_words)\n",
    "paired_words_frq.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.3: Filter Phrases Based on Part-of-Speech Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T23:53:16.924298Z",
     "start_time": "2018-08-05T23:53:16.865980Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_pairs(tokens_paired, tokens_original, pos_original):\n",
    "    \"\"\"modify (in-place) tokens_paired\n",
    "    \"\"\"\n",
    "    skip = 0        # to help track current word index while filtering\n",
    "    to_remove = []  # indices of word tokens to be remvoed\n",
    "    \n",
    "    for i in range(len(tokens_paired)):\n",
    "        word = tokens_paired[i]\n",
    "        if '__' in word:   # indicates phrase, i.e. paired words\n",
    "            num_paired = word.count('__') + 1   # number of words in phrase\n",
    "            if num_paired > 3:     # Case 1: > 3 words paired -> ignore pairing\n",
    "                skip = handle_failed_pairing(i, skip, num_paired, tokens_original, tokens_paired, to_remove)\n",
    "                continue\n",
    "            elif num_paired == 2:  # Case 2: bigrams: noun/adj, noun\n",
    "                # part-of-speech tags of 1st and 2nd words\n",
    "                pos_1, pos_2 = pos_original[i + skip: i + skip + 2]\n",
    "                if pos_1 not in ('NOUN', 'PROPN', 'ADJ') or pos_2 not in ('NOUN', 'PROPN'):\n",
    "                    skip = handle_failed_pairing(i, skip, num_paired, tokens_original, tokens_paired, to_remove)\n",
    "                    continue\n",
    "            elif num_paired == 3:  # Case 3: trigrams: noun/adj, all types, noun/adj\n",
    "                pos_1, pos_2, pos_3 = pos_original[i + skip: i + skip + 3]\n",
    "                if not all(pos in ['NOUN', 'PROPN', 'ADJ'] for pos in [pos_1, pos_3]):\n",
    "                    skip = handle_failed_pairing(i, skip, num_paired, tokens_original, tokens_paired, to_remove)\n",
    "                    continue\n",
    "            skip += num_paired - 1\n",
    "            \n",
    "    # remove rejected pairs of words that have been split and added back individually\n",
    "    if len(to_remove) > 0:\n",
    "        for j in sorted(to_remove, reverse=True):\n",
    "            del tokens_paired[j]\n",
    "\n",
    "\n",
    "def handle_failed_pairing(i, skip, num_paired, tokens_original, tokens_paired, to_remove):\n",
    "    # split up paired words failing our format requirements and update skip\n",
    "    to_remove.extend([i])\n",
    "    tokens_paired.extend(tokens_original[i + skip: i + skip + num_paired])\n",
    "    skip += num_paired - 1\n",
    "    return skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T23:54:18.585422Z",
     "start_time": "2018-08-05T23:54:17.309627Z"
    }
   },
   "outputs": [],
   "source": [
    "unigram_pos = [pos for index, tokens, pos in unigram_sents_pos]\n",
    "# perform in-place filtering of phrases in trigram_sentences\n",
    "# get rid of paired words from the corpus which\n",
    "# (1) have more than 3 words joined\n",
    "# (2) bigrams not in the format: noun/adj, noun\n",
    "# (3) trigrams not in the format: noun/adj, all types, noun/adj\n",
    "for i in range(len(trigram_sentences)):\n",
    "    filter_pairs(tokens_paired=trigram_sentences[i], \n",
    "                 tokens_original=unigram_sentences[i], \n",
    "                 pos_original=unigram_pos[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the updated phrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T23:55:27.856994Z",
     "start_time": "2018-08-05T23:55:27.464658Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('last__year', 1830),\n",
       " ('new__york', 1244),\n",
       " ('united__states', 1156),\n",
       " ('last__week', 856),\n",
       " ('per__cent', 802),\n",
       " ('white__house', 717),\n",
       " ('more__information', 683),\n",
       " ('high__school', 655),\n",
       " ('social__medium', 603),\n",
       " ('long__term', 590),\n",
       " ('north__korea', 554),\n",
       " ('last__month', 535),\n",
       " ('president__donald__trump', 525),\n",
       " ('los__angeles', 482),\n",
       " ('local__story', 475),\n",
       " ('fourth__quarter', 450),\n",
       " ('free__30__day', 425),\n",
       " ('real__estate', 416),\n",
       " ('third__quarter', 415),\n",
       " ('south__korea', 374)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paired_words = (word for sent in trigram_sentences for word in sent if '__' in word)\n",
    "paired_words_frq = Counter(paired_words)\n",
    "paired_words_frq.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.4: Final Clean-up: Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T23:57:51.717923Z",
     "start_time": "2018-08-05T23:57:49.288107Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_clean(word):\n",
    "    \"\"\"Returns a Boolean indicator of whether the input word can be considered to be 'clean'.\n",
    "    To be considered clean, a word must:\n",
    "    1. Not be '-PRON-', which is an artificial lemma representing pronouns, and\n",
    "    2. Not be an English stop word.\n",
    "    \"\"\"\n",
    "    if word == '-PRON-':\n",
    "        status = False\n",
    "    # check if word is in SpaCy's default English stop words\n",
    "    elif word in nlp.Defaults.stop_words:\n",
    "        status = False\n",
    "    else:\n",
    "        status = True\n",
    "    return status\n",
    "\n",
    "trigram_sentences = [[word for word in sent if is_clean(word)] for sent in trigram_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T23:58:52.427556Z",
     "start_time": "2018-08-05T23:58:51.113019Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../data/trigram_sentences.pkl', 'wb') as file:\n",
    "    pickle.dump(trigram_sentences, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T01:48:57.714501Z",
     "start_time": "2018-08-06T01:48:57.523266Z"
    }
   },
   "outputs": [],
   "source": [
    "# group the list of sentences in trigram_sentences by article index: \n",
    "# tokenized_articles format: \n",
    "# [[list of sentences (tokens) in article 1], ..., [list of sentences (tokens) in article 10000]]\n",
    "article_indices = (index for index, tokens, pos in unigram_sents_pos)\n",
    "article_ind_sent = zip(article_indices, trigram_sentences)\n",
    "\n",
    "tokenized_articles = []\n",
    "for index, group in groupby(article_ind_sent, key=lambda x: x[0]):\n",
    "    sentence_in_group = (sent for index, sent in group)\n",
    "    tokenized_articles.append(list(sentence_in_group))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.5: Boosting the Articles' Titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The titles of news articles generally capture the crux of their content.  Hence, it is reasonable to boost the importance of the word tokens in the articles' titles to help tease out the topic discussed in the articles' main text.\n",
    "\n",
    "In this section, the word tokens in each article's title have been 'boosted' by repeating them several times.  The number of times the title tokens are repeated is parameterized by the number of tokens in the article's main text.\n",
    "\n",
    "The first sentence of each article in `trigram_sentences` in its title - because in the first step in the tokenization section (`B.1`), each article's title was joined (prefixed) to its text.  Note that a very small number of articles have multiple sentences in their title.  Since this number is very small (< 5%), these articles have not been addressed separately - basically, only the tokens in the first sentecne of their title have been boosted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T01:49:40.048337Z",
     "start_time": "2018-08-06T01:49:40.037771Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_title_multiplier(article):\n",
    "    \"\"\"Return the number of times the input article's title \n",
    "    tokens must be repeated such that the title becomes about \n",
    "    half as long as the article's text.\n",
    "    \"\"\"\n",
    "    title = article[0]\n",
    "    # flatten list of sentence tokens in text:\n",
    "    text = list(chain.from_iterable(article[1:]))\n",
    "    if len(title) != 0:\n",
    "        # if the text has zero tokens, num_repeat = 1\n",
    "        num_repeat = max(len(text) // len(title) // 2, 1)\n",
    "    else:\n",
    "        # the title may have zero tokens if they got filtered \n",
    "        # out in the pre-processing steps upto this point\n",
    "        num_repeat = 1\n",
    "    return num_repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T01:49:50.882688Z",
     "start_time": "2018-08-06T01:49:50.593569Z"
    }
   },
   "outputs": [],
   "source": [
    "# modify tokenized_articles by boosting the number of times title tokens appear\n",
    "for i in range(len(tokenized_articles)):\n",
    "    # no. of times the article tokens must be repeated:\n",
    "    num_repeat = get_title_multiplier(tokenized_articles[i])\n",
    "    tokenized_articles[i][0] *= num_repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T02:13:08.816026Z",
     "start_time": "2018-08-06T02:13:08.511812Z"
    }
   },
   "outputs": [],
   "source": [
    "# finally, flatten the sub-lists within tokenized_articles\n",
    "# format after flattening: [[all tokens in article 1], ..., [all tokens in article 10000]]\n",
    "tokenized_articles = [list(chain.from_iterable(article_sents)) for article_sents in tokenized_articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T02:14:17.811849Z",
     "start_time": "2018-08-06T02:14:16.611664Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../data/tokenized_articles.pkl', 'wb') as file:\n",
    "    pickle.dump(tokenized_articles, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C: Construct a Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T02:33:42.259885Z",
     "start_time": "2018-08-06T02:33:39.137425Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(184929 unique tokens: ['1,000', '10', '1950', '2005', '2016']...)\n"
     ]
    }
   ],
   "source": [
    "# learn the vocabulary of the corpus by iterating over all articles\n",
    "vocab_dictionary = Dictionary(tokenized_articles)\n",
    "print(vocab_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary contains a large number of tokens.  However, tokens that are very frequent or very rare are not useful and can be removed.  In the code below, tokens that appear in less than `20` articles are removed, along with tokens that appear in more than `50%` of the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T02:33:51.469880Z",
     "start_time": "2018-08-06T02:33:51.099119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(8639 unique tokens: ['1,000', '10', '1950', '2005', '2016']...)\n"
     ]
    }
   ],
   "source": [
    "vocab_dictionary.filter_extremes(no_below=20, no_above=0.4)\n",
    "# remove gaps in word id sequence caused by token removal\n",
    "vocab_dictionary.compactify()\n",
    "print(vocab_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary size has now dropped down to `8639` unique tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T02:35:38.638260Z",
     "start_time": "2018-08-06T02:35:38.623946Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_dictionary.save('../data/vocab_dictionary.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
